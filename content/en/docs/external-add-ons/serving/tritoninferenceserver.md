+++
title = "NVIDIA Triton Inference Server"
description = "Model serving with Triton Inference Server"
weight = 50
                    
+++
{{% alert title="Out of date" color="warning" %}}
This guide contains outdated information pertaining to Kubeflow 1.0. This guide
needs to be updated for Kubeflow 1.1.
{{% /alert %}}

Kubeflow currently doesn't have a specific guide for NVIDIA Triton Inference 
Server. Note that Triton was previously known as the TensorRT Inference Server.
See the [NVIDIA
documentation](https://github.com/NVIDIA/triton-inference-server/tree/master/deploy/single_server)
for instructions on running NVIDIA inference server on Kubernetes.

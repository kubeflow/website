+++
title = "Component"
description = "Conceptual overview of components in Kubeflow Pipelines"
weight = 20
+++


A **pipeline component** is the fundamental building block for an ML engineer to construct a Kubeflow Pipelines [pipeline][pipeline]. The component structure serves the purpose of packaging a functional unit of code along with its dependencies, so that it can be run as part of a workflow in a Kubernetes environement. Components can be combined in a [pipeline][pipeline] that creates a repeatable workflow, with individual components coordinating on inputs and outputs like parameters and [artifacts][artifacts].

A component is similar to a programming function. Indeed, it is most often implemented as a wrapper to a Python function using the [KFP Python SDK][KFP SDK]. However, a KFP component goes further than a simple function, with support for code dependencies, runtime environments, and distributed execution requirements.

KFP components are designed to simplify constructing and running ML workflows in a Kubernetes environment. Using KFP components engineers can iterate faster, reduce maintenance overhead, and focus more attention on ML work.

## The Why Behind KFP Components

Running ML code in a Kubernetes cluster presents many challenges. Some of the main challenges are:
- Managing **code dependencies** (Python libraries and versions)
- Handling **system dependencies** (OS-level packages, GPU drivers, runtime environments)
- **Building and maintaining container images** and everything around this from container registry support to CVE (Common Vulnerabilities and Exposures) fixes
- **Deploying supporting resources** like PersistentVolumeClaims and ConfigMaps
- Handling **inputs and outputs** including metadata, parameters, logs, and artifacts
- **Ensuring compatibility** across clusters, images, and dependencies

KFP components simplify these challenges by enabling ML engineers to:
- **Stay at the Python level** - where most modern ML work occurs
- **Iterate quickly** – modify code without creating or rebuilding containers at each step
- **Focus on ML tasks** - rather than on platform and infrastructure concerns
- **Work seamlessly with Python IDE tools** – enable debugging, syntax highlighting, type checking, and docstring usage
- **Move between environments** – transition from local development to distributed execution with minimal changes

## What Does a Component Consist Of?

A KFP component consist of the following key elements:

### 1. Code
- Typically a Python function, but can be other code such as a Bash command.

### 2. Dependency Support
- **Python libraries** - to be installed at runtime
- **Environment variables** - to be available in the runtime environment
- **Python package indices** - (for example, private PyPi servers) if needed to support installations
- **Cluster resources** - to support use of ConfigMaps, Secrets, PersistentVolumeClaims, and more
- **Runtime dependencies** - to support CPU, memory, and GPU requests and limits

### 3. Base Image
- Defines the base container runtime environment
- May include system dependencies and pre-installed Python libraries

### 4. Input/Output (I/O) Specification
- Individual components cannot share in-memory data with each other, so they use the following concepts to support exchanging information and publishing results:
  - **Parameters** – for small values
  - **[Artifacts][artifacts]** - for larger data like model files, processed datasets, and metadata

## Constructing a Component

### 1. Python-Based Components
The recommended way to define a component is using the `@dsl.component` decorator from the [KFP Python SDK][KFP SDK]. Below are two basic component definition examples:
```python
from kfp.dsl import component, Output, Dataset

# hello component
@component()
def hello_world(name: str = "World") -> str:
    print(f"Hello {name}!")
    return name

# process data component
@component(
    base_image="python:3.12-slim-bookworm",
    packages_to_install=["pandas>=2.2.3"],
)
def process_data(output_data: Output[Dataset]):
    '''Get max from an array'''
    import pandas as pd
   # create dataset to write to output
    data = pd.DataFrame(data=[[1,2,3],[4,5,6]], columns=["a","b","c"])
    data.to_csv(output_data.path)
```

Observe that these are wrapped Python functions. The `@component` wrapper helps the KFP Python SDK supply the needed context for running these functions in containers as part of a KFP [pipeline][pipeline].

The `hello_world` component just uses default behavior (run the Python function on the default base image, which is `python:3.9`). The `process_data` component adds layers of customization, by supplying the name of a specific `base_image`, and `packages_to_install`. This component also uses KFP's `Output[Dataset]` class, which takes care of creating a KFP [artifact][artifact] type output.

Note that inputs and outputs are defined as Python function parameters. Also, dependencies can often be installed at runtime, avoiding the need for custom base containers. Python-based components give close access to the Python tools that ML experimenters rely on, like modules and imports, usage information, type hints, and debugging tools.

Provided that dependencies are satisfied in your environment, it is also easy to run Python-based components as simple Python functions, which can be useful for local work. For example, to run `process_data` as a Python function try:
``` python
# Provide path as dataset type (as the function expects)
dataset = Dataset(uri="data.csv")
# execute the function
# (writes data to data.csv locally)
process_data.execute(output_data=dataset)
# access the underlying function docstring
print(process_data.python_func.__doc__)
```

Component usage can get much more complex, as AI/ML use-cases often have demanding code and environment dependencies. For more on Python-based components, see the [component][python-sdk-component] SDK documentation.


### 2. YAML-Based Components

The KFP backend uses YAML-based definitions to specify components. While the [KFP Python SDK][KFP SDK] can do this conversion automatically when a Python-based [pipeline][pipeline] is submitted, some use-cases can benefit from the direct YAML-based component approach.

A YAML-based component definition has the following parts:

* **Metadata:** name, description, etc.
* **Interface:** input/output specifications (name, type, description, default value, etc).
* **Implementation:** A specification of how to run the component given a set of argument values for the component’s inputs. The implementation section also describes how to get the output values from the component once the component has finished running.

YAML-based components support system commands directly. Here is simple YAML-based component example:
```yaml
# my_component.yaml file
name: my-component
description:

inputs:
- {name: string prefix, type: String}
- {name: num, type: Integer}

outputs:

implementation:
  container:
    image: python:3.12-slim-bookworm
    args:
    - echo
    - {inputValue: string prefix}
    - ...
    - {inputValue: num}
```

For the complete definition of a YAML-based component, see the [component specification][yaml-component].

YAML-based components can be loaded for use in the Python SDK alongside Python-based components:
```python
from kfp.components import load_component_from_file

my_comp = load_component_from_file("my_component.yaml")
```

Note that a component loaded from a YAML-based component will not have the same level of Python support that Python-based components do (like executing the function locally).

<!-- TODO: Briefly discuss graph components, container components, and importer components (see sdk dsl scripts) -->

## "Containerize" a Component

The KFP command-line tool contains a build command to help users "containerize" a component. This can be used to create the `Dockerfile`, `runtime-dependencies.txt`, and other supporting files, or even to build the custom image and push it to a registry. In order to use this utility, the `target_image` parameter must be set in the Python-based component definition, which itself is saved in a file.
```bash
# build Dockerfile and runtime-dependencies.txt
kfp component build --component-filepattern the_component.py --no-build-image --platform linux/amd64 .
```
Note that creating and maintaining custom containers can carry a significant maintenance burden. In general, a 1-to-1 relationship between components and containers is not needed or recommended, as AI/ML work is often highly iterative. A best practice is to work with a small set of base images that can support many components. If you need more control over the container build than the `kfp` CLI provides, consider using a container CLI like [docker][docker-cli] or [podman][podman-cli].

## Next steps

* Read an [overview of Kubeflow Pipelines](/docs/components/pipelines/overview/).
* Follow the [pipelines quickstart guide](/docs/components/pipelines/getting-started/) 
  to deploy Kubeflow and run a sample pipeline directly from the Kubeflow 
  Pipelines UI.
* Build your own 
  [component and pipeline](/docs/components/pipelines/legacy-v1/sdk/component-development/).
* Build a [reusable component](/docs/components/pipelines/legacy-v1/sdk/component-development/) for
  sharing in multiple pipelines.


[pipeline]: /docs/components/pipelines/concepts/pipeline
[KFP SDK]: https://kubeflow-pipelines.readthedocs.io
[artifacts]: /docs/components/pipelines/concepts/output-artifact
[python-sdk-component]: https://kubeflow-pipelines.readthedocs.io/en/stable/source/dsl.html#kfp.dsl.component
[yaml-component]: /docs/components/pipelines/reference/component-spec
[docker-cli]: https://github.com/docker/cli
[podman-cli]: https://github.com/containers/podman

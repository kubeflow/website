+++
title = "NVIDIA Triton Inference Server"
description = "Model serving with Triton Inference Server"
weight = 50
+++

Kubeflow currently doesn't have a specific guide for NVIDIA Triton Inference 
Server. Note that Triton was previously known as the TensorRT Inference Server.
See the [NVIDIA
documentation](https://github.com/NVIDIA/triton-inference-server/tree/master/deploy/single_server)
for instructions on running NVIDIA inference server on Kubernetes.

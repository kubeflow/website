{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of build-pipelines.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pxRW-6f_uNq"
      },
      "source": [
        "# Building a Pipeline\n",
        "\n",
        "## Overview\n",
        "\n",
        "This tutorial describes how to create a component for Kubeflow Pipelines and how to combine components into a pipeline. For an easier start, experiment with [the Kubeflow Pipelines samples](https://www.kubeflow.org/docs/pipelines/tutorials/build-pipeline/).\n",
        "\n",
        "\n",
        "## Before you begin\n",
        "\n",
        "1. Run the following command to install Kubeflow Pipelines SDK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04mM73j7nWJ-"
      },
      "source": [
        "!pip install kfp --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KExWR1i_7Ur"
      },
      "source": [
        "2. Import the `kfp` and `kfp.components` packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLAhMbMG_M3A"
      },
      "source": [
        "import kfp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IARdgYktshE8"
      },
      "source": [
        "3. Create an instance of the [kfp.Client class](https://kubeflow-pipelines.readthedocs.io/en/latest/source/kfp.client.html#kfp.Client)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ4YU9N_sos6"
      },
      "source": [
        "# If you run this command on a Jupyter notebook running on Kubeflow, you can\n",
        "# exclude the host parameter.\n",
        "# client = kfp.Client()\n",
        "client = kfp.Client(host='<your-kubeflow-pipelines-host-name>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deBrhgzrD3Fr"
      },
      "source": [
        "## Understanding pipelines\n",
        "\n",
        "### Component\n",
        "\n",
        "A pipeline component is an implementation of a pipeline task. A component represents a step in the workflow. Each component takes zero or more inputs and may produce zero or more outputs. A component consists of an interface (inputs/outputs), the implementation (a Docker container image and command-line arguments) and metadata (name, description).\n",
        "\n",
        "### Task\n",
        "\n",
        "A pipeline task is an instance of a component. In KFP DSL, a task is a [`ContainerOp`](https://kubeflow-pipelines.readthedocs.io/en/latest/source/kfp.dsl.html#kfp.dsl.ContainerOp) object.\n",
        "\n",
        "### Pipeline\n",
        "\n",
        "A pipeline is a description of a machine learning (ML) workflow, including all of the components of the workflow and how they work together. The pipeline includes the definition of the inputs (parameters) required to run the pipeline and the inputs and outputs of each component.\n",
        "\n",
        "## Designing your pipeline\n",
        "\n",
        "When designing your pipeline, think about how to split an ML workflow into pipeline steps. In general you should design your components with composability in mind. Think about upstream and downstream components. What formats to consume as inputs from the upstream components. What formats to use for output data so that downstream components can consume it.\n",
        "\n",
        "You can learn more about how to build components and reusable components from [compoents in KFP GitHub repo](\n",
        "https://github.com/kubeflow/pipelines/tree/master/components).\n",
        "\n",
        "### Code sample\n",
        "\n",
        "In the following example, we have a Python function that downloads from a public website a tarball (`.tar.gz` file) that contains multiple `.csv` files, and merge these `.csv` files into a single file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vn9MXolH_2BG"
      },
      "source": [
        "def download_and_merge_csv(url: str, output_csv: str):\n",
        "  import glob\n",
        "  import pandas as pd\n",
        "  import tarfile\n",
        "  import urllib.request\n",
        "\n",
        "  with urllib.request.urlopen(url) as res:\n",
        "    tarfile.open(fileobj=res, mode=\"r|gz\").extractall('data')\n",
        "  df = pd.concat(\n",
        "      [pd.read_csv(csv_file, header=None) \n",
        "       for csv_file in glob.glob('data/*.csv')])\n",
        "  df.to_csv(output_csv, index=False, header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWmF17kyIKGF"
      },
      "source": [
        "We can test the function using the sample code and data shown below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he6MK5x1Fwbk"
      },
      "source": [
        "download_and_merge_csv(\n",
        "    url='https://storage.googleapis.com/ml-pipeline-playground/iris-csv-files.tar.gz', \n",
        "    output_csv='merged_data.csv')\n",
        "\n",
        "!head merged_data.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT6Di92BOrNQ"
      },
      "source": [
        "Next, we want to run this script in the Kubeflow Pipelines system.\n",
        "\n",
        "While we can keep it a single step in a pipeline, for the demo purpose, we can refactor the function by striping away the web download logic and reuse the existing [Web Download component](https://github.com/kubeflow/pipelines/blob/master/components/web/Download/component.yaml) available from Kubeflow Pipelines GitHub repo. \n",
        "\n",
        "Other than removing the file download logic, we also type annotate the function parameters with [`kfp.components.InputPath`](https://kubeflow-pipelines.readthedocs.io/en/latest/source/kfp.components.html?highlight=inputpath#kfp.components.InputPath) and [`kfp.components.OutputPath`](https://kubeflow-pipelines.readthedocs.io/en/latest/source/kfp.components.html?highlight=outputpath#kfp.components.OutputPath) respectively. The [`kfp.components.InputPath`](https://kubeflow-pipelines.readthedocs.io/en/latest/source/kfp.components.html?highlight=inputpath#kfp.components.InputPath) annotation informs the system to pass the data file path to the function instead of the actual data. This is necessary to match the output from the upstream component given [Web Download component](https://github.com/kubeflow/pipelines/blob/master/components/web/Download/component.yaml) outputs the file path as well.\n",
        "Similarly, [`kfp.components.OutputPath`](https://kubeflow-pipelines.readthedocs.io/en/latest/source/kfp.components.html?highlight=outputpath#kfp.components.OutputPath) tells the systems to output the file path instead of the file content.\n",
        "\n",
        "Learn more about component data passing (TODO: link)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB3eNHmNCN2C"
      },
      "source": [
        "def merge_csv(file_path: kfp.components.InputPath('Tarball'),\n",
        "              output_csv: kfp.components.OutputPath('CSV')):\n",
        "  import glob\n",
        "  import pandas as pd\n",
        "  import tarfile\n",
        "\n",
        "  tarfile.open(name=file_path, mode=\"r|gz\").extractall('data')\n",
        "  df = pd.concat(\n",
        "      [pd.read_csv(csv_file, header=None) \n",
        "       for csv_file in glob.glob('data/*.csv')])\n",
        "  df.to_csv(output_csv, index=False, header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WATSaHZkJySg"
      },
      "source": [
        "We then call [`kfp.components.create_component_from_func()`](https://kubeflow-pipelines.readthedocs.io/en/latest/source/kfp.components.html#kfp.components.create_component_from_func) to convert the function into a pipeline component.\n",
        "[`kfp.components.create_component_from_func()`](https://kubeflow-pipelines.readthedocs.io/en/latest/source/kfp.components.html#kfp.components.create_component_from_func) returns a factory function that you can use to create a [`kfp.dsl.ContainerOp`](https://kubeflow-pipelines.readthedocs.io/en/latest/source/kfp.dsl.html#kfp.dsl.ContainerOp) class instance -- a pipeline task.\n",
        "Optionally, as demonstrated in the code below, you can save the [component specification](https://www.kubeflow.org/docs/pipelines/reference/component-spec/) into a reusable YMAL file which can be loaded later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDVQ8QjWOniD"
      },
      "source": [
        "merge_csv_op = kfp.components.create_component_from_func(\n",
        "    func=merge_csv,\n",
        "    output_component_file='component.yaml', # This is optional. It saves the component spec for future use.\n",
        "    base_image='python:3.7',\n",
        "    packages_to_install=['pandas==1.1.4'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9Axem9HPHP2"
      },
      "source": [
        "## Building your pipeline\n",
        "\n",
        "We build a pipeline that uses the above component and the [Web Download component](https://github.com/kubeflow/pipelines/blob/master/components/web/Download/component.yaml). \n",
        "\n",
        "We can load the [Web Download component](https://github.com/kubeflow/pipelines/blob/master/components/web/Download/component.yaml) from URL using [`kfp.components.load_component_from_url()`](https://kubeflow-pipelines.readthedocs.io/en/latest/source/kfp.components.html?highlight=load_component_from_url#kfp.components.load_component_from_url). The function return a factory function, similar to `merge_csv_op`, that we can use to instantiate a pipeline task.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDzFCaGQa_oR"
      },
      "source": [
        "web_downloader_op = kfp.components.load_component_from_url(\n",
        "    'https://raw.githubusercontent.com/kubeflow/pipelines/master/components/web/Download/component.yaml')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4bIwiHhbACy"
      },
      "source": [
        "Next, we define a pipeline function that uses these factory functions.\n",
        "A pipeline function defines a collection of tasks and the dependencies among the tasks if there's any. \n",
        "In the example below, our pipeline contains two tasks: `web_downloader_task` and `merge_csv_task`. \n",
        "\n",
        "`web_downloader_task` gets its input value, the url to the Tarball file from the function parameter, aka pipeline parameter, while its output is passed as input to `merge_csv_task`. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsyKJeBOTlkz"
      },
      "source": [
        "# Define a pipeline and create a task from a component:\n",
        "def my_pipeline(url):\n",
        "  web_downloader_task = web_downloader_op(url=url)\n",
        "  merge_csv_task = merge_csv_op(file=web_downloader_task.outputs['data'])\n",
        "  # The outputs of the merge_csv_task can be referenced using the\n",
        "  # merge_csv_task.outputs dictionary: merge_csv_task.outputs['output_csv']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnhZm12y_wvc"
      },
      "source": [
        "Learn more about advanced features: \n",
        "- [DSL Recursion](https://www.kubeflow.org/docs/pipelines/sdk/dsl-recursion/)\n",
        "- [Data passing](#) (TODO: link)\n",
        "- [Manipulate Kubernetes Resources as Part of a Pipeline](https://www.kubeflow.org/docs/pipelines/sdk/manipulate-resources/) (Experimental)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT3O_2GgVKoT"
      },
      "source": [
        "## Compiling and running your pipeline\n",
        "\n",
        "After defining the pipeline in Python as described above, you can compile the pipeline to an workflow YAML spec before you submit it to the Kubeflow Pipelines service. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0Ll8ve2WNUo"
      },
      "source": [
        "kfp.compiler.Compiler().compile(\n",
        "    pipeline_func=my_pipeline,\n",
        "    package_path='pipeline.yaml')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNLI1-_bfEky"
      },
      "source": [
        "We can submit the compiled workflow specification `pipeline.yaml` using the SDK client we initiated earlier. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRNHZpfnVJ0h"
      },
      "source": [
        "client.create_run_from_pipeline_package(\n",
        "    pipeline_file='pipeline.yaml',\n",
        "    arguments={\n",
        "        'url': 'https://storage.googleapis.com/ml-pipeline-playground/iris-csv-files.tar.gz'\n",
        "    })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFqF9MF6gDvi"
      },
      "source": [
        "Alternatively, you can upload `pipeline.yaml` through the Kubeflow Pipelines UI. See the guide to [getting started with the UI](https://www.kubeflow.org/docs/pipelines/pipelines-quickstart)."
      ]
    }
  ]
}
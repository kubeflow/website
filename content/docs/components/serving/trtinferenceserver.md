+++
title = "NVIDIA TensorRT Inference Server"
description = "Model serving with TRT Inference Server"
weight = 50
+++

Kubeflow currently doesn't have a specific guide for NVIDIA TensorRT Inference 
Server. See the [NVIDIA 
documentation](https://github.com/NVIDIA/tensorrt-inference-server/tree/master/deploy/single_server)
for instructions on running NVIDIA inference server on Kubernetes.

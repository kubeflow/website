+++
title = "Build Reusable Components"
description = "A detailed tutorial on creating components that you can use in various pipelines"
weight = 3
+++

TODO: ADD LINKS TO DEFINITIONS OF PIPELINE< COMPONENT, ETC, WHEN PR IS MERGED.

This page describes how to author a reusable component that you can
load and run in Kubeflow Pipelines. A reusable component is a pre-implemented
standalone component that is easy to add as a step in any pipeline.

Below is a summary of the steps involved in creating and using a component:

1.  Write the program that contains your component's logic and
    containerize the program. The program needs to use specific methods to
    pass data to and from the component, as described in this guide.
1.  Write the component specification file in YAML format that describes the
    component for the Kubeflow Pipelines system.
1.  Use the Kubeflow Pipelines SDK to load and run the component in your 
    pipeline.

The rest of this page gives detailed descriptions of the above steps.

## Passing the data to and from the containerized program

When you want to write a component you need to think about the component
communicates with upstream and downstream components. That is, how it consumes
input data and produces output data.

### Input data

There are several ways to make input data available to a program running inside
a container:

1.  Small data: Pass the data content as a command-line argument:  
program.py --param 100  
    This way is only suitable for small pieces of data (<< 512 KiB).
1.  Bigger data: Current version of the pipelines system cannot
    automatically help bring the bigger data to the container running the
    program. So, right now the program (or the wrapper script) should receive
    data URIs instead of the data itself and then access the data pointed by
the URIs:  
    ` program.py --train-uri
    [https://server.edu/datasets/1/train.tsv](https://server.edu/datasets/1/train.tsv)
     --eval-uri
[https://server.edu/datasets/1/eval.tsv](https://server.edu/datasets/1/train.tsv)  
 program.py --train-gcs-uri gs://bucket/datasets/1/train.tsv  
     program.py --big-query-table my_table`  

### Output data

The program needs to write the output data to some location and the system must
also be informed about those locations so that the system can pass the data
between steps.  
Output data should be written to paths provided as command line arguments (i.e.
paths should not be hardcoded). As a default, we recommend using GCS as the
general storage solution for writing output. However, the component author
decides which storage solution to pick, to write the output. For structured data
the component author can use BigQuery. The user of the component will have to
provide the specific URI/path, or table name, to which to write the results.  
The program should upload the data to some storage system, then pass out a URI
pointing to the data (by writing that URI to a file and instructing the system
to pick it up and treat as the value of a particular component output).  
`program.py --out-model-uri gs://bucket/163/output_model --out-model-uri-file
/outputs/output_model_uri/data`  
Note that the program here accepts both a URI for uploading and a file path to
write that URI to.  
Why should the program output the URI it has just received as an input argument?
The reason is that the URIs specified in the pipeline are usually not the real
URIs, but rather URI templates that contain UIDs that are only resolved at
runtime when the containerized program is started. Example of such URI:
`‘gs://my-bucket/{{workflow.uid}}/{{pod.id}}/data'`. The containerized program
is the only one who sees the fully-resolved URI. If we want another (downstream)
components to access that URI, the program needs to somehow pass it forward. The
only way to do it is to write that URI to a file and let the system grab it and
seamlessly deliver it to the downstream components.  
Note that in cases where the program cannot control the resulting URI/ID of the
created object (e.g. in cases where this URI is generated by the outside
system), the program should just accept the file path to write the resulting
URI/ID:  

```python
program.py --out-model-uri-file /outputs/output_model_uri/data
```

## Building a component

### Choosing the data passing strategy:

#### Small data:

Inputs: Read value from command line argument.  
Outputs: Write value to a local file (whose path is provided as command-line
argument).

#### Bigger data (or storage-specific component):

Inputs:** Read data URI **from a file provided as command-line argument. Then
**read the data** from that URI.  
Outputs: Upload data to the URI provided as command-line argument. Then write
that URI to a local file (whose path is also provided as command-line
argument).

### Future-proofing the program code

If the program has access to the tensorflow package, you can use
`tensorflow.gfile `to read and write files. It supports both local and GCS
paths.

If you cannot use `tensorflow.gfile`:  
To avoid the need to modify the program code in the near future or have
different versions for different storage systems, a solution is to write a
program that uses the local files to read inputs and write outputs and then add
a storage-specific wrapper that downloads and uploads the data from/to a
specific storage solution like GCS. This way there might be multiple wrappers
for different storage systems, or the wrapper can be removed if it's no longer
needed.

## Writing the program code

This example program will have two inputs (for small and big data) and one
output.  
Here we use Python3 as a programming language of choice.  
We're using`
[tensorflow.gfile](https://www.tensorflow.org/api_docs/python/tf/gfile)` package
to read/write data that can be either in local files or on GCS/S3. It's a
preferred way to do the reading/writing. If you do not have access to the
tensorflow package than you should create a wrapper script that uses `[gsutil
cp](https://cloud.google.com/storage/docs/gsutil/commands/cp)` command to
download the input data before running the main program and upload the output
data after the program finishes.

### program.py:

    #!/usr/bin/env python3
    import argparse
    import os
    from pathlib import Path
    from tensorflow import gfile #Supports both local paths and GCS/S3

    #Function doing the actual work
    def do_work(input1_file, output1_file, param1):
      for x in range(param1):
        line = next(input1_file)
        if not line:
          break
        _ = output1_file.write(line)

    #Defining and parsing the command-line arguments
    parser = argparse.ArgumentParser(description='My program description')
    parser.add_argument('--input1-path', type=str, help='Path of the local file or GCS blob containing the Input 1 data.')
    parser.add_argument('--param1', type=int, default=100, help='Parameter 1.')
    parser.add_argument('--output1-path', type=str, help='Path of the local file or GCS blob where the Output 1 data should be written.')
    parser.add_argument('--output1-path-file', type=str, help='Path of the local file where the Output 1 URI data should be written.')
    args = parser.parse_args()

    gfile.MakeDirs(os.path.dirname(args.output1_path))
    #Opening the input/output files and perform the actual work
    with gfile.Open(args.input1_path, 'r') as input1_file, gfile.Open(args.output1_path, 'w') as output1_file:
        do_work(input1_file, output1_file, args.param1)

    #Writing args.output1_path to a file so that it will be passed to downstream tasks
    Path(args.output1_path_file).parent.mkdir(parents=True, exist_ok=True)
    Path(args.output1_path_file).write_text(args.output1_path)

The command-line of this program is:

    python3 program.py --input1-path <URI to Input 1 data> --param1 <value of Param1 input> --output1-path <URI for Output 1 data> --output1-path-file <local file path for the Output 1 URI>

We need to pass the "URI for Output 1 data" forward so that the downstream steps
can access it. This might become unnecessary in the future, but for now the
system needs it. To pass that URI the program needs to write it to a local file
and tell the system to grab it and expose as an output. Where should the program
write that URI? We need to avoid hard-coding any paths, so the program receives
that local file path through the `--output1-path-file` command-line argument
value.

## Writing Dockerfile

The instructions on creating container images are not specific to KF Pipelines. 
This section is merely providing standard container creation instructions as a
convenience to the user. Component authors are free to use any procedure of
their choice, to create the docker containers.  
The Dockerfile must contain all program code (including the wrapper) and its
dependencies (OS packages, python packages etc).  
Ensure you have write access to some container registry where you'll be pushing
the container image (e.g. Google Container Registry or DockerHub).Think of a
name for your container image. Let's use the
'[gcr.io/my-org/my-image](http://gcr.io/my-org/my-image)' name in this doc.

### Example Dockerfile:

    ARG BASE_IMAGE_TAG=1.12.0-py3
    FROM tensorflow/tensorflow:$BASE_IMAGE_TAG
    RUN python3 -m pip install keras
    COPY ./src /pipelines/component/src

Create the `build_image.sh` script to build the container image based on the
Dockerfile and push it to some container repository.  
Run build_image.sh script to build the container image based on the Dockerfile
and push it to some container repository.  
Best practice: Get the strict image name with digest for the just pushed image
and use it for reproducibility.

### Example build_image.sh:

    #!/bin/bash -e
    image_name=gcr.io/my-org/my-image #Specify the image name here
    image_tag=latest
    full_image_name=${image_name}:${image_tag}
    base_image_tag=1.12.0-py3

    cd "$(dirname "$0")" 
    docker build --build-arg BASE_IMAGE_TAG=$base_image_tag -t "$full_image_name" .
    docker push "$full_image_name"

    #Output the strict image name (which contains the sha256 image digest)
    docker inspect --format="{{index .RepoDigests 0}}" "$IMAGE_NAME"

Don't forget to make your script executable (`chmod +x build_image.sh`)

## Writing component definition file

Knowing the full schema of the component spec file is not required since this
tutorial provides enough information for most of the components. See the
[document](http://go/kfp_components) for the description of the format. See the
[schema](https://github.com/kubeflow/pipelines/pull/669/files#diff-ef88b94c0d0097eeba570b3d7c64aa21)
(+ outline) for the formal specification.   
Start writing the component definition (component.yaml) by specifying the
container image in the component's implementation section:

    implementation:
      container:
        image: gcr.io/my-org/my-image@sha256:a172..752f #Container image name taken that you've pushed to a container repo.

Start writing the component definition by looking at the (wrapper) program
command-line and populating the component's implementation section:  
 

    implementation:
      container:
        image: gcr.io/my-org/my-image@sha256:a172..752f
        #command is a list of strings (command-line arguments). YAML language has two syntaxes for lists and you can use either of them. Here we use the "flow syntax" - comma-separated strings inside square brackets.
        command: [
          python3, /kfp/component/src/program.py, #Path of the program inside container
          --input1-path, <URI to Input 1 data>,
          --param1, <value of Param1 input>,
          --output1-path, <URI template for Output 1 data>,
          --output1-path-file, <local file path for the Output 1 URI>,
        ]

The command-line still contains some dummy placeholders (in angle brackets).
Let's replace them with real placeholders. Placeholder represents a command-line
argument that will be replaced with some value or path before the program is
executed. In component.yaml, the placeholders are specified using the YAML's map
syntax to distinguish them from the verbatim strings. There are three
placeholders available:

+   `{inputValue: Some input name}`   
    This placeholder will be replaced by the **value** of the argument to the
    specified input. Useful for small pieces of input data.
+   `{outputPath: Some output name}`   
    This placeholder will be replaced by auto-generated **local path**, that
    where the program should write its output data. Instructs the system read
    the content of the file and store it as the value of the specified output.

While we're putting real placeholders in the command-line, we need to add
corresponding input and output specifications to the inputs and outputs
sections. The input/output specification contains the input name, type,
description and default value. Only the name is required. The input and output
names are free-form strings, but be careful with the YAML syntax and use quotes
if necessary. The input/output names do not need to be the same as the
command-line flags which are usually quite short.  
   
Let's start replacing the placeholders one by one:

+   Replace <URI to Input 1 file> with `{inputValue: Input 1 URI}` and
    add "Input 1 URI" to the inputs section. URLs are small, so we're passing
    them in as command-line arguments.
+   Replace <value of Param1 input> with `{inputValue: Parameter 1}` and add
    "Parameter 1" to the inputs section. Integers are small, so we're passing
    them in as command-line arguments.
+   Replace <URI template for Output 1 file> with `{inputValue: Output 1 URI
    template}` and add "Output 1 URI template" to the *inputs* section. This
    looks very confusing - we're adding an output URI into the inputs section.
    The reason is that currently the user must manually pass in URIs, so this
    is input, not output.
+   Replace <local file path for the Output 1 URI> with `{outputPath: Output
    1 URI}` and add "Output 1 URI" to the *outputs* section. Again, this looks
    quite confusing - we now have both input and output called "Output 1 URI"
    (we could use different names though). The reason if that the URI is
    "pass-through". It gets passed to the task as input and is then output from
    the task, so that downstream tasks have access to it.

After replacing the placeholders and adding inputs/outputs, our component.yaml
looks like this:

    inputs: #List of input specs. Each input spec is a map.
    - {name: Input 1 URI}
    - {name: Parameter 1}
    - {name: Output 1 URI template}
    outputs:
    - {name: Output 1 URI}
    implementation:
      container:
        image: gcr.io/my-org/my-image@sha256:a172..752f
        command: [
          python3, /pipelines/component/src/program.py,
          --input1-path,
          {inputValue: Input 1 URI}, #Refers to the "Input 1 URI" input
          --param1,
          {inputValue: Parameter 1}, #Refers to the "Parameter 1" input
          --output1-path,
          {inputValue: Output 1 URI template}, #Refers to "Output 1 URI template" input
          --output1-path-file,
          {outputPath: Output 1 URI}, #Refers to the "Output 1 URI" output
        ]

This component specification is sufficient, but we should add more metadata, to
make it more useful.  
Let's add component name and description.  
For each input and output let's add the description, default value (optional)
and type. //See the curated types list for the list of suggested names for
common types.  
   
Final version of component.yaml:  
 

    name: Do dummy work
    description: Performs some dummy work.
    inputs:
    - {name: Input 1 URI, type: GCSPath, description='GCS path to Input 1'}
    - {name: Parameter 1, type: Integer, default='100', description='Parameter 1 description'} #The default values must be specified as YAML strings.
    - {name: Output 1 URI template, type: GCSPath, description='GCS path template for Output 1'}
    outputs:
    - {name: Output 1 URI, type: GCSPath, description='GCS path for Output 1'}
    implementation:
      container:
        image: gcr.io/my-org/my-image@sha256:a172..752f
        command: [
          python3, /pipelines/component/src/program.py,
          --input1-path,       {inputValue: Input 1 URI},
          --param1,            {inputValue: Parameter 1},
          --output1-path,      {inputValue: Output 1 URI template},
          --output1-path-file, {outputPath: Output 1 URI},
        ]

## Using the components

The main purpose of a component is to be used to compose a pipeline.  
Here is a sample pipeline that shows how to load component and use it to compose
a pipeline  
 

    import kfp
    #Load the component by calling load_component_from_file or load_component_from_url
    #To load the component, the pipeline author only needs to have access to the component.yaml file. The Kubernetes cluster executing the pipeline will need access to the container image specified in the component.
    dummy_op = kfp.components.load_component_from_file(os.path.join(component_root, 'component.yaml')) 
    #dummy_op = kfp.components.load_component_from_url('http://....../component.yaml')

    #dummy_op is now a "factory function" that accepts the arguments for the component's inputs and produces a task object (e.g. ContainerOp instance).
    #Inspect the dummy_op function in Jupyter Notebook by typing "dummy_op(" and pressing Shift+Tab
    #You can also get help by writing help(dummy_op) or dummy_op? or dummy_op??
    #The signature of the dummy_op function corresponds to the inputs section of the component. ! Some tweaks are performed to make the signature valid and pythonic:
    #1) All inputs with default values will come after the inputs without default values
    #2) The input names are converted to pythonic names (spaces and symbols replaced with underscores and letters lowercased).

    #Define a pipeline and create a task from a component:
    @kfp.dsl.pipeline(name='My pipeline', description='')
    def my_pipeline():
        dummy1_task = dummy_op(
            input_1_uri='gs://my-bucket/datasets/train.tsv', #Input name "Input 1 GCS URI" got converted to pythonic parameter name "input_1_uri"
            parameter_1='100',
            output_1_uri='gs://my-bucket/{{workflow.uid}}/{{pod.name}}/output_1/data', #Currently, the pipeline author must use Argo placeholders ("{{workflow.uid}}" and "{{pod.name}}") to guarantee that the outputs from different pipeline runs and tasks write to unique locations and do not overwrite each other.
        ).apply(kfp.gcp.use_gcp_secret('user-gcp-sa')) #To access GCS, we must configure the container to have access to a GCS secret that grants required access to the bucket.
        #The outputs of the dummy1_task can be referenced using the dummy1_task.outputs dictionary. ! At this moment, the output names are converted to lowercased dashed names.

        #Pass the outputs of the dummy1_task to some other component
        dummy2_task = dummy_op(
            input_1_uri=dummy1_task.outputs['output-1-uri'],
            parameter_1='200',
            output_1_uri='gs://my-bucket/{{workflow.uid}}/{{pod.name}}/output_1/data',
        ).apply(kfp.gcp.use_gcp_secret('user-gcp-sa')) #To access GCS, we must configure the container to have access to a GCS secret that grants required access to the bucket.
    #This pipeline can be compiled, uploaded and submitted for execution.

## Organizing the component files

This section provides a recommended way to organize the component files. There
is no requirement that the files must be organized this way. Using the standard
organization though makes it possible to reuse the same scripts for testing,
image building and component versioning.  
See the
[sample component](https://github.com/Ark-kun/pipelines/tree/Added-sample-component/components/sample/keras/train_classifier)
for an real-life component example.

components/<component group>/<component name>/

    src/*            #Component source code files
    tests/*          #Unit tests
    run_tests.sh     #Small script that runs the tests
    README.md        #Documentation. Move to docs/ if multiple files needed

    Dockerfile       #Dockerfile to build the component container image
    build_image.sh   #Small script that runs docker build and docker push

    component.yaml   #Component definition in YAML format

# Best practices

## General component design rules

+   Components must be designed with composability in mind. Think about
    upstream and downstream components. What formats to consume as inputs from
    the upstream components. What formats to use for output data so that
    downstream components can consume it.
+   Component code must use local files for input/output data (Unless
    impossible  - e.g. CMLE, BigQuery require GCS staging paths).
+   Components must be "pure" - not use any outside data except data that
    comes through inputs (unless impossible). Everything should either be
    inside the container or come from inputs. Network access is strongly
    discouraged unless that's the explicit purpose of a component (e.g.
    upload/download).

## Writing component code

+   The program must be runnable both locally and inside the Docker
    container.
+   Programming languages:

    +   Generally, use the language that makes the most sense. If the
        component wraps a Java library, then it might make sense to use Java to
        expose that library.
    +   For most new components when the performance is not a concern
        using the Python language is preferred (use version 3 wherever possible).
    +   If a component wraps an existing program, it's preferred to
        directly expose the program in the component command-line.
    +   If there needs to be some wrapper around the program (small
        pre-processing or post-processing like file renaming), it can be done
        with a shell script.
    +   Follow the best practices for the chosen language

+   Each output data piece should be written to a separate file (+see next line)
+   The input and output file paths must be passed in the command-line and
    not hard-coded:

    +   Typical command-line:  
        program.py --input-data <input path> --output-data <output path> --param 42
    +   Do NOT hardcode paths in the program:

        +   open("/output.txt", "w")

+   For temporary data you should use library functions that create
    temporary files. (e.g. for python use
    [https://docs.python.org/3/library/tempfile.html](https://docs.python.org/3/library/tempfile.html)
    ) Do not just write to the root or testing will be hard.
+   Design the code to be testable

## Writing tests

+   Follow the general rules section so that writing the tests is easier.
+   Use the unit testing libraries that are standard for the language you're
    using.
+   Try to design the component code so that it can be tested using unit tests

    +   Do not use network unless necessary

+   Prepare small input data files so that the component code can be tested
    in isolation.

    +   E.g. for an ML prediction component prepare a small model and
        evaluation dataset.

+   Use testing best practices.

    +   Test the expected behavior of the code, not just verify that
        "nothing has changed"

        +   For training you can look at loss at final iteration
        +   For predicting you can look at the result metrics
        +   For data augmenting you can check for some desired post-invariants

+   If the component cannot be tested locally or in isolation, then create a
    small PoC pipeline that tests the component. You can use conditionals to
    verify the output values of a particular task and only enable the "success"
    task if the results are expected.

    +   In future the pipeline team might provide improved testing
        tools that will be able to remove some of the boilerplate code of such
        testing or enable local container testing using Docker.

## Writing Dockerfile

+   Follow the best practices:
    [https://docs.docker.com/develop/develop-images/dockerfile_best-practices/](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/)

+   Structure the dockerfile so that the required packages are installed
    first and main component scripts/binaries are added last. Ideally, split
    the Dockerfile in two parts (base image and component code) so that the
    main component image build is fast and more reliable (not require network
    access).

## Writing component.yaml file

See the
[document](https://docs.google.com/document/d/1TsdikmvL7utUzAd2fjrnVhzjVrP6a-edHBqQlc7DnOQ/edit?usp=sharing)
for the description of the format. You can look at the definitions for the
[existing components](https://github.com/kubeflow/pipelines/search?q=filename%3Acomponent.yaml&unscoped_q=filename%3Acomponent.yaml)
in the main repository for some general examples on the container component yaml
format.

+   Use `{inputValue: Input name}` command-line placeholder for small
    values that should be directly inserted into the command-line.
+   Use `{outputPath: Output name}` command-line placeholder for output file
    locations.
+   Specify full command-line in ‘command:' instead of just arguments to the
    entry-point.